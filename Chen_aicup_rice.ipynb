{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "%env SM_FRAMEWORK=tf.keras\n",
    "import segmentation_models as sm\n",
    "import tensorflow as tf\n",
    "# import keras.callbacks\n",
    "# from keras import backend as K\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "import albumentations\n",
    "import random\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import PIL.Image as Image  \n",
    "\n",
    "\n",
    "\n",
    "# os.environ['KERAS_BACKEND']='tensorflow'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "seed= 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normal(img):\n",
    "    img = (img-0)/255  \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(img_path,label_path):\n",
    "    img_name = os.listdir(img_path)\n",
    "    label_name = os.listdir(label_path)\n",
    "    \n",
    "    img_all = []\n",
    "    label_all = []\n",
    "    img_name_all = []\n",
    "    \n",
    "    for img_tmp in img_name:\n",
    "        img = cv2.imread(img_path + '/' + img_tmp)\n",
    "        label = cv2.imread(label_path + '/' + img_tmp,0)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         #resize\n",
    "#         resize_number = 256\n",
    "#         img = cv2.resize(img,(resize_number,resize_number))\n",
    "#         label = cv2.resize(label,(resize_number,resize_number))\n",
    "# #         img = minmax_normal(img)\n",
    "# #         label = minmax_normal(label)\n",
    "        label = label.reshape(label.shape[0],label.shape[1],1)\n",
    "        img_all.append(img)\n",
    "        label_all.append(label)\n",
    "        img_name_all.append(img_tmp)\n",
    "        \n",
    "    return np.array(img_all),np.array(label_all),img_name_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(train_x,train_y):\n",
    "\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    \n",
    "    transform = albumentations.Compose([\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "#         albumentations.ChannelShuffle(p=0.5),\n",
    "        albumentations.Blur(blur_limit=3, p=0.5),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        albumentations.ElasticTransform(alpha_affine=15, border_mode=1, p=0.5),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=20,\n",
    "                                        interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_REPLICATE, p=0.5)\n",
    "    ])\n",
    "    train_x = train_x.astype(np.float32)\n",
    "    train_y = train_y.astype(np.float32)\n",
    "    for i in range(len(train_x)):\n",
    "        \n",
    "        #new_x.append(train_x[i])\n",
    "        #new_y.append(train_y[i])\n",
    "        \n",
    "        \n",
    "        aug= transform(image= train_x[i], mask= train_y[i])\n",
    "#         print(aug)\n",
    "#         print(type(aug))\n",
    "        new_x.append(aug['image'])\n",
    "        new_y.append(aug['mask'])\n",
    "        \n",
    "        \n",
    "    return np.array(new_x).astype(np.float64), np.array(new_y).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    ''' Dice Coefficient \n",
    "    Args:\n",
    "        y_true (np.array): Ground Truth Heatmap (Label)\n",
    "        y_pred (np.array): Prediction Heatmap\n",
    "    '''\n",
    "    class_num = 1\n",
    "    for i in range(class_num):\n",
    "        y_true_f = K.flatten(y_true[:,:,:,i])\n",
    "        y_pred_f = K.flatten(y_pred[:,:,:,i])\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "        if i == 0:\n",
    "            total_loss = loss\n",
    "        else:\n",
    "            total_loss = total_loss + loss\n",
    "            total_loss = total_loss / class_num\n",
    "    return total_loss\n",
    "    \n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    ''' Dice Coefficient Loss\n",
    "    Args:\n",
    "        y_true (np.array): Ground Truth Heatmap (Label)\n",
    "        y_pred (np.array): Prediction Heatmap\n",
    "    '''\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_net():\n",
    "#     classes means unet output channel numbers\n",
    "    model = sm.Unet('efficientnetb0', classes = 1, input_shape=(None,None,3), encoder_weights='imagenet', activation='sigmoid')\n",
    "    \n",
    "#     tensorflow.kerasiioptimizers.Adadelta(learning_rate=0.01)\n",
    "    \n",
    "    model.compile(\n",
    "    'sgd',\n",
    "    loss= sm.losses.bce_jaccard_loss,\n",
    "    metrics=[dice_coef],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_cudnn_error():\n",
    "#     gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#     if gpus:\n",
    "#         try:\n",
    "#             # Currently, memory growth needs to be the same across GPUs\n",
    "#             for gpu in gpus:\n",
    "#                 tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#             logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#             print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#         except RuntimeError as e:\n",
    "#             # Memory growth must be set before GPUs have been initialized\n",
    "#             print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_code = 27\n",
    "epoch = 50\n",
    "best_iou = 0 \n",
    "\n",
    "train_iou = []\n",
    "train_loss =[]\n",
    "val_iou = []\n",
    "val_loss = []\n",
    "\n",
    "model = u_net()\n",
    "img_path = 'Train_Dev/crop_training_256'\n",
    "label_path = 'Train_Dev/crop_trainlabel_image_256'\n",
    "\n",
    "\n",
    "# read data and normalization\n",
    "img_all, label_all, img_name_all = read_data(img_path,label_path)\n",
    "img_all = minmax_normal(img_all)\n",
    "label_all = minmax_normal(label_all)\n",
    "\n",
    "print('data_shape = ' + str(img_all.shape))\n",
    "\n",
    "#data augmentation\n",
    "vali_percentage= 0.15\n",
    "# #     vali_x, vali_y= img_all[-vali_count:], label_all[-vali_count:]\n",
    "# # train_x, train_y= img_all[:], label_all[:]\n",
    "train_x_name, vali_x_name = train_test_split(img_name_all, test_size = vali_percentage, random_state = 42)\n",
    "train_x, vali_x = train_test_split(img_all, test_size = vali_percentage, random_state = 42)\n",
    "train_y, vali_y = train_test_split(label_all, test_size = vali_percentage, random_state = 42)\n",
    "\n",
    "del img_all,label_all,img_name_all\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    ep = i+1\n",
    "    print('epoch:'+ str(ep))\n",
    "    \n",
    "    new_x, new_y = data_aug(train_x, train_y)\n",
    "    \n",
    "# #     # checkpoint\n",
    "# #     filepath = \"weight/weights_exp\"+ str('%02d'%exp_code) +\"-{epoch:02d}.h5\"\n",
    "# #     checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor = 'val_dice_coef', verbose=1, save_best_only = False,save_weights_only = False)\n",
    "# #     callbacks_list = [checkpoint]\n",
    "\n",
    "    train_history = model.fit(new_x, new_y, validation_data=(vali_x, vali_y), epochs = 1, batch_size= 5, verbose= 2)\n",
    "#     train_history = model.fit(train_x, train_y, epochs = 1, batch_size= 9, verbose= 2)\n",
    "\n",
    "    val_iou.append( train_history.history['val_dice_coef'][-1] )\n",
    "    train_iou.append( train_history.history['dice_coef'][-1] )\n",
    "    val_loss.append( train_history.history['val_loss'][-1] )\n",
    "    train_loss.append( train_history.history['loss'][-1] )\n",
    "\n",
    "    print('epoch: ', ep, ', ', 'val_IOU= ', val_iou[-1])\n",
    "#     print('epoch: ', ep, ', ', 'train_IOU= ', train_iou[-1])\n",
    "\n",
    "    print('val_IOU= ', val_iou[-1]) \n",
    "    print('train_IOU= ', train_iou[-1])\n",
    "    print('val_LOSS= ', val_loss[-1]) \n",
    "    print('train_LOSS= ', train_loss[-1]) \n",
    "    print()\n",
    "\n",
    "    if val_iou[-1] > best_iou:\n",
    "        model.save('weight/best_model_exp' + str('%02d'%exp_code) + 'epoch_' + str('%02d'%ep) + '.h5')\n",
    "        best_iou = val_iou[-1]\n",
    "        print('model save at epoch: '+ str(ep) +', val_dice_coef: '+str(val_iou[-1]))\n",
    "        \n",
    "#     if train_iou[-1] > best_iou:\n",
    "#         model.save('weight/best_model_exp' + str('%02d'%exp_code) + 'epoch_' + str('%02d'%ep) + '.h5')\n",
    "#         best_iou = train_iou[-1]\n",
    "#         print('model save at epoch: '+ str(ep) +', val_dice_coef: '+str(train_iou[-1]))\n",
    "        \n",
    "    del train_history,new_x,new_y\n",
    "    gc.collect()\n",
    "#     del new_x,new_y\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output record file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'exp_result/csv/exp_' + str('%02d'%exp_code) +'_history.csv'\n",
    "exp_result = pd.DataFrame(list(zip(train_iou, val_iou, train_loss, val_loss)),\n",
    "               columns =['train_iou', 'val_iou','train_loss','val_loss'])\n",
    "exp_result.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot loss\n",
    "filename = 'exp_result/figure/exp_' + str('%02d'%exp_code) +'_loss.jpeg'\n",
    "plt.figure(figsize=(15,10))\n",
    "x_ticks = np.linspace(1,epoch,epoch)\n",
    "# y_ticks = np.linspace(0,1,11)\n",
    "plt.plot(x_ticks,val_loss,color='r',label='val_loss')\n",
    "plt.plot(x_ticks,train_loss,color='b',label='train_loss')\n",
    "plt.title('Exp_' + str('%02d'%exp_code) +' loss', fontsize=20)\n",
    "plt.xlabel('Epoch',fontsize=15)\n",
    "plt.ylabel('loss',fontsize=15)\n",
    "\n",
    "plt.xlim((1,epoch))\n",
    "# plt.ylim((0,1))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "# plt.yticks(y_ticks)\n",
    "plt.grid()\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Dice\n",
    "filename = 'exp_result/figure/exp_' + str('%02d'%exp_code) +'_dice.jpeg'\n",
    "plt.figure(figsize=(15,10))\n",
    "x_ticks = np.linspace(1,epoch,epoch)\n",
    "y_ticks = np.linspace(0,1,11)\n",
    "plt.plot(x_ticks,val_iou,color='r',label='val_dice')\n",
    "plt.plot(x_ticks,train_iou,color='b',label='train_dice')\n",
    "plt.title('Exp_' + str('%02d'%exp_code) +' dice_coefficient',fontsize=20)\n",
    "plt.xlabel('Epoch',fontsize=15)\n",
    "plt.ylabel('dice_coefficient',fontsize=15)\n",
    "\n",
    "plt.xlim((1,epoch))\n",
    "plt.ylim((0,1))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "plt.yticks(y_ticks)\n",
    "plt.grid()\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedDropout(tf.keras.layers.Dropout):\n",
    "    def _get_noise_shape(self, inputs):\n",
    "        if self.noise_shape is None:\n",
    "            return self.noise_shape\n",
    "        symbolic_shape = K.shape(inputs)\n",
    "        noise_shape = [symbolic_shape[axis] if shape is None else shape for axis, shape in enumerate(self.noise_shape)]\n",
    "        return tuple(noise_shape)\n",
    "    \n",
    "model2 = tf.keras.models.load_model('weight/best_model_exp' + str('%02d'%exp_code) +'epoch_19'+ '.h5', compile=True, \n",
    "                                    custom_objects={'swish':tf.compat.v2.nn.swish,\n",
    "                                                  'binary_crossentropy_plus_jaccard_loss':sm.losses.bce_jaccard_loss,\n",
    "                                                  'dice_coef_loss':dice_coef_loss,\n",
    "                                                  'dice_coef': dice_coef,\n",
    "                                                  'FixedDropout':FixedDropout})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "from skimage.filters import threshold_otsu\n",
    "import matplotlib.patches as mpatches\n",
    "from skimage.morphology import square, dilation, erosion\n",
    "import time\n",
    "\n",
    "def transform_center(img, ans):\n",
    "    \n",
    "    ans= ans*255\n",
    "\n",
    "    ans= ans.reshape(ans.shape[0],ans.shape[1])\n",
    "    try:\n",
    "        thresh = threshold_otsu(ans)\n",
    "    except:\n",
    "        return ''\n",
    "    bw = dilation(ans > thresh, square(11)) #11\n",
    "    bw = erosion(bw > thresh, square(11))\n",
    "    label_image = label(bw, connectivity = 2)\n",
    "    plt.imshow(bw)\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(img.reshape(img.shape[0],img.shape[1],3), cmap= 'gray')\n",
    "    \"\"\"\n",
    "    \n",
    "    # 紀錄所有區域之面積\n",
    "    area=[]\n",
    "    for region in regionprops(label_image):\n",
    "        minr, minc, maxr, maxc = region.bbox\n",
    "        x_width= maxc - minc\n",
    "        y_width= maxr - minr\n",
    "        area.append(x_width*y_width)\n",
    "    # 排序大小\n",
    "    index_large= sorted(range(len(area)), key = lambda k : area[k])\n",
    "    index_large.reverse()\n",
    "    region_prob= []\n",
    "    for i in range(len(regionprops(label_image))):\n",
    "        region_prob.append( regionprops(label_image)[index_large[i]] )\n",
    "    \n",
    "    #region_prob= region_prob[:int(len(region_prob)*0.9)]\n",
    "        \n",
    "        \n",
    "    all_center= []\n",
    "    print('region_bbox: ', len(region_prob))\n",
    "    for region in region_prob:\n",
    "        minr, minc, maxr, maxc = region.bbox\n",
    "        x_width= maxc - minc\n",
    "        y_width= maxr - minr\n",
    "        minr= minr-y_width/2\n",
    "        minc= minc-x_width/2\n",
    "        x_width= x_width*2\n",
    "        y_width= y_width*2\n",
    "        \n",
    "        \n",
    "        if (maxc - minc)>0 and (maxr - minr)>0:\n",
    "            all_center.append([int(minc+x_width/2), int(minr+y_width/2)])\n",
    "            \"\"\"\n",
    "            rect = mpatches.Rectangle((minc, minr), x_width, y_width,\n",
    "                                        fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "    #plt.show()\n",
    "    \n",
    "    return all_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def read_data_predict(img_path):\n",
    "    img_name = os.listdir(img_path)\n",
    "    img_all = []\n",
    "    \n",
    "    for img_tmp in img_name:\n",
    "        img = cv2.imread(img_path + '/' + img_tmp)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         #resize\n",
    "#         resize_number = 256\n",
    "#         img = cv2.resize(img,(resize_number,resize_number))\n",
    "        \n",
    "        img_all.append(img)\n",
    "    return np.array(img_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#load data\n",
    "img_path = 'test/crop_test_private_256_hist/'\n",
    "test_img = read_data_predict(img_path)\n",
    "test_img = minmax_normal(test_img)\n",
    "\n",
    "# predict\n",
    "predict_mask = model2.predict(test_img,batch_size = 16)\n",
    "\n",
    "# resize\n",
    "resize_number = 256 \n",
    "threshold = 0.5\n",
    "print('orginal size:',predict_mask.shape)\n",
    "process_mask = [cv2.resize(predict_mask[i],(resize_number,resize_number)) for i in range(len(predict_mask))]\n",
    "process_mask = np.array(process_mask)\n",
    "process_mask_t = process_mask > threshold\n",
    "process_mask_t = process_mask_t.astype(float)\n",
    "print('after preocss size:',process_mask_t.shape)\n",
    "\n",
    "# output\n",
    "# 1.先找圖片名字\n",
    "img_name_path = 'test_private/'\n",
    "img_list = os.listdir(img_name_path)\n",
    "ful_img_name = [img_list[i].split('.')[0]for i in range(len(img_list))]\n",
    "\n",
    "# 2.讀入所有圖片\n",
    "spt_img_name = os.listdir(img_path)\n",
    "cnt=0\n",
    "\n",
    "# 3.針對圖片做輸出\n",
    "for img_name in ful_img_name:\n",
    "    print(img_name)\n",
    "    if(len(img_name)<10):\n",
    "        o_x = 3000\n",
    "        o_y = 2000\n",
    "        small_x = math.ceil(o_x / resize_number)\n",
    "        small_y = math.ceil(o_y / resize_number)\n",
    "        small_img_number = small_x * small_y\n",
    "        full_img = Image.new('L',(small_x * resize_number , small_y * resize_number))\n",
    "    else:\n",
    "        o_x = 2304\n",
    "        o_y = 1728\n",
    "        small_x = math.ceil(o_x / resize_number)\n",
    "        small_y = math.ceil(o_y / resize_number)\n",
    "        small_img_number = small_x * small_y\n",
    "        full_img = Image.new('L',(small_x * resize_number , small_y * resize_number))\n",
    "    cnt = 0\n",
    "    for y in range(small_y):\n",
    "        for x in range(small_x):\n",
    "            cnt = cnt+1\n",
    "            img_code = img_name  + '_' + str(cnt)  +'.JPG'\n",
    "            idx = spt_img_name.index(img_code)\n",
    "#             print(spt_img_name[idx])\n",
    "#             print(y,x)\n",
    "            small_img = Image.fromarray(process_mask_t[idx])\n",
    "            full_img.paste(small_img,(x * resize_number,y *  resize_number))\n",
    "    if(len(img_name)<10): \n",
    "        ans = np.array(full_img)\n",
    "    else:\n",
    "        ans = np.array(full_img) \n",
    "        ans = ans[0:1728,0:2304]\n",
    "#     ans = np.array(full_img)\n",
    "    ans_img = cv2.imread(img_name_path + img_name +'.JPG')\n",
    "    bbox = transform_center(ans_img,ans)\n",
    "    radius = 5\n",
    "    for box in bbox:\n",
    "        cv2.rectangle(ans_img,\n",
    "                      (box[0]-radius, box[1]-radius),\n",
    "                      (box[0]+radius, box[1]+radius),\n",
    "                      (255, 255, 255),\n",
    "                      -1)\n",
    "    for x in range(0,o_x,resize_number):\n",
    "        for y in range(0,o_y,resize_number):\n",
    "            cv2.rectangle(ans_img,\n",
    "                      (x-radius, y-radius),\n",
    "                      (x+radius, y+radius),\n",
    "                      (0, 0, 255),\n",
    "                      2)      \n",
    "#     save image\n",
    "    cv2.imwrite('predict_test_private/exp_' + str('%02d'%exp_code) +'/img/'+ img_name +'.JPG',ans_img) #save labelimage\n",
    "#     save csv file\n",
    "    filename_1 = 'predict_test_private/exp_' + str('%02d'%exp_code) +'/csv/' + img_name +'.csv'\n",
    "    filename_2 = 'predict_upload/exp_' + str('%02d'%exp_code) +'/' + img_name +'.csv'\n",
    "    exp_result = pd.DataFrame(bbox , columns =['X','Y'])\n",
    "    exp_result.to_csv(filename_1,header = None, index = False)\n",
    "    exp_result.to_csv(filename_2,header = None, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare label and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label_name = os.listdir('Train_Dev/train_labels')\n",
    "\n",
    "radius = 1\n",
    "for label_name in all_label_name[0:1]:\n",
    "    # read image\n",
    "    img = cv2.imread('Train_Dev/training/' + label_name.split('.')[0] + '.JPG')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    label = pd.read_csv('Train_Dev/train_labels/' + label_name, header= None)# read label\n",
    "    label_img = np.zeros(img.shape, dtype = 'uint8')#make label image\n",
    "    \n",
    "    bbox = label.values\n",
    "    for box in bbox:\n",
    "        cv2.rectangle(label_img,\n",
    "                      (box[0]-radius, box[1]-radius),\n",
    "                      (box[0]+radius, box[1]+radius),\n",
    "                      (255, 255, 255),\n",
    "                      -1)\n",
    "    cv2.imwrite('Train_Dev/train_labels_image/'+ label_name.split('.')[0] +'.JPG',label_img) #save labelimage\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.imshow(label_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "resize_number=256\n",
    "\n",
    "o_x=3000\n",
    "o_y=2000\n",
    "bounds = []\n",
    "for x in range(0,o_x,resize_number):\n",
    "    for y in range(0,o_y,resize_number):\n",
    "        bound = [x,y]\n",
    "        bounds.append(bound)\n",
    "# for box in bounds:\n",
    "#         cv2.rectangle(ans_img,\n",
    "#                       (box[0]-rr, box[1]-rr),\n",
    "#                       (box[0]+rr, box[1]+rr),\n",
    "#                       (0, 0, 255),\n",
    "#                       2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3000,256):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_number*()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
